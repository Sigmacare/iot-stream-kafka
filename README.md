# **IoT-Stream-Kafka**  
**Real-time IoT Sensor Data Processing & Storage using Kafka, InfluxDB, and MongoDB**  

## **ğŸ“Œ Overview**  
**IoT-Stream-Kafka** is a microservice-based application designed to handle real-time sensor data ingestion, processing, and storage. It leverages **Apache Kafka** for high-throughput data streaming, **InfluxDB** for fast time-series storage, and **MongoDB** for storing processed alerts.

## **ğŸ”§ Tech Stack**  
- **Backend:** Node.js (Express.js)  
- **Streaming:** Kafka (KafkaJS)  
- **Database:** InfluxDB (Sensor Data), MongoDB (Alerts)  
- **Message Broker:** Apache Kafka    

---

## **ğŸ“‚ Project Structure**  
```
iot-stream-kafka/
â”‚â”€â”€ consumers/         # Kafka Consumers
â”‚   â”œâ”€â”€ storageConsumer.js  # Stores sensor data in InfluxDB
â”‚   â”œâ”€â”€ processingConsumer.js  # Processes data & generates alerts
â”‚
â”‚â”€â”€ config/            # Configurations
â”‚   â”œâ”€â”€ kafka.js       # Kafka setup
â”‚   â”œâ”€â”€ influxdb.js    # InfluxDB connection
â”‚   â”œâ”€â”€ mongodb.js     # MongoDB connection
â”‚
â”‚â”€â”€ routes/            # API Routes
â”‚   â”œâ”€â”€ sensorRoutes.js # Handles incoming sensor data
â”‚
â”‚â”€â”€ models/            # Mongoose models
â”‚   â”œâ”€â”€ Alert.js       # Alert schema for MongoDB
â”‚
â”‚â”€â”€ .env               # Environment variables
â”‚â”€â”€ server.js          # Main server file
â”‚â”€â”€ package.json       # Dependencies
â”‚â”€â”€ README.md          # Documentation
```

---

## **ğŸš€ Features**  
âœ… High-throughput IoT data ingestion using **Kafka**  
âœ… Fast time-series storage with **InfluxDB**  
âœ… Real-time processing & alert generation using **Kafka consumers**  
âœ… Fault-tolerant **MongoDB** storage for alerts  
âœ… **Scalable architecture** for handling millions of IoT events  

---

## **ğŸ› ï¸ Setup & Installation**  

### **1ï¸âƒ£ Clone the Repository**  
```bash
git clone https://github.com/yourusername/iot-stream-kafka.git
cd iot-stream-kafka
```

### **2ï¸âƒ£ Install Dependencies**  
```bash
npm install
```

### **3ï¸âƒ£ Setup Environment Variables**  
Create a `.env` file and add:  
```env
# Server
PORT=5000

# Kafka
KAFKA_BROKER=localhost:9092
KAFKA_CLIENT_ID=iot-stream
KAFKA_SENSOR_TOPIC=sensor-data
KAFKA_ALERT_TOPIC=alerts
KAFKA_CONSUMER_GROUP_STORAGE=sensor-storage-group
KAFKA_CONSUMER_GROUP_PROCESSING=sensor-processing-group

# InfluxDB
INFLUX_URL=your_influxdb_url
INFLUX_TOKEN=your_influx_token
INFLUX_ORG=your_org
INFLUX_BUCKET=sensor_data

# MongoDB
MONGO_URI=mongodb://localhost:27017/iot_alerts
```

---

## **ğŸ“¡ Running the Application**  

### **1ï¸âƒ£ Start Kafka (If not using Aiven Kafka)**  

#### **Using Docker (Recommended)**  
```bash
docker-compose up -d
```

#### **Manual Setup (Without Docker)**  
1. Start **Zookeeper**  
   ```bash
   bin/zookeeper-server-start.sh config/zookeeper.properties
   ```
2. Start **Kafka Broker**  
   ```bash
   bin/kafka-server-start.sh config/server.properties
   ```

---

### **2ï¸âƒ£ Start the Server**  
```bash
npm start
```
or  
```bash
npm run dev  # For development mode
```

---

## **ğŸ“ API Endpoints**  

### **1ï¸âƒ£ Store Sensor Data**  
**Endpoint:** `POST /sensor`  
**Request Body:**  
```json
{
  "device_code": "1",
  "accelX": 0.02,
  "accelY": -0.03,
  "accelZ": 9.81,
  "gyroX": 0.01,
  "gyroY": -0.02,
  "gyroZ": 0.03,
  "heartRate": 85,
  "oxygen": 98
}
```
**Response:**  
```json
{
  "message": "Data received & queued for processing"
}
```

---

## **ğŸ“Š Database Schema**  

### **InfluxDB Schema (sensor_data bucket)**  
| Field      | Type    | Description              |
|------------|--------|--------------------------|
| deviceId   | Tag    | Unique sensor ID         |
| accelX     | Float  | Accelerometer X value    |
| accelY     | Float  | Accelerometer Y value    |
| accelZ     | Float  | Accelerometer Z value    |
| gyroX      | Float  | Gyroscope X value        |
| gyroY      | Float  | Gyroscope Y value        |
| gyroZ      | Float  | Gyroscope Z value        |
| heartRate  | Float  | Heart rate of patient    |
| oxygen     | Float  | Oxygen saturation level  |
| timestamp  | Time   | Event timestamp          |

### **MongoDB Schema (Alerts Collection)**  
```js
const mongoose = require('mongoose');

const alertSchema = new mongoose.Schema({
  device_code: String,
  alert_type: String,
  message: String,
  timestamp: { type: Date, default: Date.now }
});

module.exports = mongoose.model("Alert", alertSchema);
```

---

## **ğŸ’¡ How Kafka Works in This Project**  

1ï¸âƒ£ **Producer:** The IoT devices send data to Kafkaâ€™s `sensor-data` topic.  
2ï¸âƒ£ **Consumer Group 1 (Storage):** One consumer saves the data into **InfluxDB**.  
3ï¸âƒ£ **Consumer Group 2 (Processing):** Another consumer **processes sensor data**, detects anomalies, and stores alerts in **MongoDB**.  
4ï¸âƒ£ **Alerts:** If abnormal values are detected, an alert is generated and stored in **MongoDB**.  

---

## **ğŸš€ Deployment**  


---

## **ğŸ“ˆ Monitoring Data**  

### **Check Data in MongoDB**
```bash
mongosh
use iot_alerts
db.alerts.find().pretty()
```

### **Check Data in InfluxDB**
```bash
influx query 'from(bucket: "sensor_data") |> range(start: -1h)'
```

---

## **ğŸ“œ License**  
This project is licensed under **MIT License**.

---

## **ğŸ’¬ Contributing**  
Feel free to **fork, improve, and open a PR**! ğŸš€

---

## **ğŸ‘¨â€ğŸ’» Authors**  
- **Your Name** Anish Pillai

---

**ğŸš€ Happy Coding!** ğŸ¯ğŸ”¥
